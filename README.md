# LLM-Resources-I-Like

## Videos

1. [Attention Mechanism in Large Language Models](https://www.youtube.com/watch?v=OxCpWwDCDFQ) - This one provides a very visual demonstration of the concept of both self/multi-headed attention


## Blogs

1. What is positional encoding? [Blog](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)
2. 

## Git Repos

1. [Medium Articles](https://github.com/ddl-wadkars/medium_articles) [Code](https://medium.com/analytics-vidhya/nlp-transformer-unit-test-95459fefbea9) a transformer from scratch.
2. 
